{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8bf402",
   "metadata": {},
   "source": [
    "# Implementation of Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11d3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./dataset/train_tfidf_features.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test_tfidf_features.csv\")\n",
    "\n",
    "X_train = train_df.drop([\"id\", \"label\"], axis=1)\n",
    "y_train = train_df[\"label\"]\n",
    "X_test = test_df.drop([\"id\"], axis=1)\n",
    "\n",
    "import os\n",
    "if not os.path.exists('./predictions'):\n",
    "    os.makedirs('./predictions')\n",
    "\n",
    "def save_predictions(y_pred, filename):\n",
    "        output = pd.DataFrame({\"id\": test_df[\"id\"], \"label\": y_pred})\n",
    "        output.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6cecc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for self-evaluation\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def f1_score(y_true, y_pred, class_label):\n",
    "    tp = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "    fp = np.sum((y_true != class_label) & (y_pred == class_label))\n",
    "    fn = np.sum((y_true == class_label) & (y_pred != class_label))\n",
    "    \n",
    "    if tp + 0.5 * (fp + fn) == 0:\n",
    "        return 0\n",
    "    \n",
    "    f1 = tp / (tp + 0.5 * (fp + fn))\n",
    "    return f1\n",
    "\n",
    "def macro_f1_score(y_true, y_pred):\n",
    "    f1_hateful = f1_score(y_true, y_pred, class_label=1)\n",
    "    f1_non_hateful = f1_score(y_true, y_pred, class_label=0)\n",
    "    return (f1_hateful + f1_non_hateful) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c221b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: 2000\n",
      "Training set accuracy: 0.7832867783985102\n",
      "F1 Score for Hateful (class 1): 0.684085510688836\n",
      "F1 Score for Non-Hateful (class 0): 0.8350752878653676\n",
      "Macro F1 Score: 0.7595803992771017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: 1000\n",
      "Training set accuracy: 0.7543063314711359\n",
      "F1 Score for Hateful (class 1): 0.6357832988267771\n",
      "F1 Score for Non-Hateful (class 0): 0.8146294344924482\n",
      "Macro F1 Score: 0.7252063666596127\n",
      "Components: 500\n",
      "Training set accuracy: 0.7371973929236499\n",
      "F1 Score for Hateful (class 1): 0.60886887233674\n",
      "F1 Score for Non-Hateful (class 0): 0.8021207606695294\n",
      "Macro F1 Score: 0.7054948165031347\n",
      "Components: 100\n",
      "Training set accuracy: 0.6881401303538175\n",
      "F1 Score for Hateful (class 1): 0.4970436414828719\n",
      "F1 Score for Non-Hateful (class 0): 0.7740058195926285\n",
      "Macro F1 Score: 0.6355247305377502\n"
     ]
    }
   ],
   "source": [
    "# PCA dimensionality reduction and logistic regression\n",
    "# 2000 components seems to be too many features as indicated in task 2, reduce to test 1000, 500 and 100 for less time complexity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "components_list = [2000, 1000, 500, 100]\n",
    "log_reg_cv = LogisticRegressionCV(cv=5, random_state=0) # max_iter 1000 no dif\n",
    "\n",
    "for n_components in components_list:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    log_reg_cv.fit(X_train_pca, y_train)\n",
    "\n",
    "    y_pred = log_reg_cv.predict(X_test_pca)\n",
    "\n",
    "    # Self evaluation on training set\n",
    "    y_train_pred = log_reg_cv.predict(X_train_pca)\n",
    "\n",
    "    print(\"Components:\", n_components)\n",
    "    print(\"Training set accuracy:\", accuracy(y_train, y_train_pred))\n",
    "    print(\"F1 Score for Hateful (class 1):\", f1_score(y_train, y_train_pred, 1))\n",
    "    print(\"F1 Score for Non-Hateful (class 0):\", f1_score(y_train, y_train_pred, 0))\n",
    "    print(\"Macro F1 Score:\", macro_f1_score(y_train, y_train_pred))\n",
    "\n",
    "    save_predictions(y_pred, f'./predictions/LogisticRegressionCV_PCA_{n_components}_components_Predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f663ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear\n",
      "Training set accuracy: 0.7314944134078212\n",
      "F1 Score for Hateful (class 1): 0.5715081723625557\n",
      "F1 Score for Non-Hateful (class 0): 0.8044915254237288\n",
      "Macro F1 Score: 0.6879998488931423\n",
      "Kernel: poly\n",
      "Training set accuracy: 0.8564944134078212\n",
      "F1 Score for Hateful (class 1): 0.7715397443023903\n",
      "F1 Score for Non-Hateful (class 0): 0.8953932298294731\n",
      "Macro F1 Score: 0.8334664870659316\n",
      "Kernel: rbf\n",
      "Training set accuracy: 0.8780842644320298\n",
      "F1 Score for Hateful (class 1): 0.8254893794252395\n",
      "F1 Score for Non-Hateful (class 0): 0.9063184724768591\n",
      "Macro F1 Score: 0.8659039259510493\n",
      "Kernel: sigmoid\n",
      "Training set accuracy: 0.6504888268156425\n",
      "F1 Score for Hateful (class 1): 0.4891988433407042\n",
      "F1 Score for Non-Hateful (class 0): 0.7343653250773994\n",
      "Macro F1 Score: 0.6117820842090518\n"
     ]
    }
   ],
   "source": [
    "# PCA dimensionality reduction and SVM, as SVMs typically perform better with high-dimensional and unstructured datasets, such as image and text data, compared to logistic regression.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pca = PCA(n_components=500)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel, random_state=0)\n",
    "    svm.fit(X_train_pca, y_train)\n",
    "    \n",
    "    y_pred = svm.predict(X_test_pca)\n",
    "\n",
    "    # Self evaluation on training set\n",
    "    y_train_pred = svm.predict(X_train_pca)\n",
    "\n",
    "    print(\"Kernel:\", kernel)\n",
    "    print(\"Training set accuracy:\", accuracy(y_train, y_train_pred))\n",
    "    print(\"F1 Score for Hateful (class 1):\", f1_score(y_train, y_train_pred, 1))\n",
    "    print(\"F1 Score for Non-Hateful (class 0):\", f1_score(y_train, y_train_pred, 0))\n",
    "    print(\"Macro F1 Score:\", macro_f1_score(y_train, y_train_pred))\n",
    "\n",
    "    save_predictions(y_pred, f'./predictions/SVM_PCA_500_components_{kernel}_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f4b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954903309638462\n",
      "F1 Score for Non-Hateful (class 0): 0.9972280949025135\n",
      "Macro F1 Score: 0.9963592129331799\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier, no tuning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set. Shows a lot of overfitting\n",
    "y_train_pred = decision_tree.predict(X_train)\n",
    "print(\"Training set accuracy:\", accuracy(y_train, y_train_pred))\n",
    "print(\"F1 Score for Hateful (class 1):\", f1_score(y_train, y_train_pred, 1))\n",
    "print(\"F1 Score for Non-Hateful (class 0):\", f1_score(y_train, y_train_pred, 0))\n",
    "print(\"Macro F1 Score:\", macro_f1_score(y_train, y_train_pred))\n",
    "\n",
    "save_predictions(y_pred, f'./predictions/DecisionTree_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7867523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier with max_depth after considering overfitting. Performs worse\n",
    "max_depth_values = [10, 100, 500]\n",
    "\n",
    "# Train and predict using Decision Tree for each max_depth value\n",
    "for max_depth in max_depth_values:\n",
    "    decision_tree = DecisionTreeClassifier(random_state=0, max_depth=max_depth)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    \n",
    "    save_predictions(y_pred, f'./predictions/DecisionTree_max_depth_{max_depth}_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7faf421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "Random Forest predictions saved to ./predictions/RandomForest_Predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Implementation, no tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred = rf.predict(X_train)\n",
    "print(\"Training set accuracy:\", accuracy(y_train, y_train_pred))\n",
    "print(\"F1 Score for Hateful (class 1):\", f1_score(y_train, y_train_pred, 1))\n",
    "print(\"F1 Score for Non-Hateful (class 0):\", f1_score(y_train, y_train_pred, 0))\n",
    "print(\"Macro F1 Score:\", macro_f1_score(y_train, y_train_pred))\n",
    "\n",
    "save_predictions(y_pred, './predictions/RandomForest_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e44d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, \n",
    "                                   n_iter=20, cv=3, scoring=scorer, verbose=2, random_state=0, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Save predictions\n",
    "save_predictions(y_pred, './predictions/RandomForest_Tuned_Predictions.csv')\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred = best_rf.predict(X_train)\n",
    "print(\"Training set accuracy:\", accuracy(y_train, y_train_pred))\n",
    "print(\"F1 Score for Hateful (class 1):\", f1_score(y_train, y_train_pred, pos_label=1))\n",
    "print(\"F1 Score for Non-Hateful (class 0):\", f1_score(y_train, y_train_pred, pos_label=0))\n",
    "print(\"Macro F1 Score:\", f1_score(y_train, y_train_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28238fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Gradient Boosting using XGBoost\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting using XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "print(\"Training set accuracy:\", accuracy(y_train, y_train_pred))\n",
    "print(\"F1 Score for Hateful (class 1):\", f1_score(y_train, y_train_pred, 1))\n",
    "print(\"F1 Score for Non-Hateful (class 0):\", f1_score(y_train, y_train_pred, 0))\n",
    "print(\"Macro F1 Score:\", macro_f1_score(y_train, y_train_pred))\n",
    "\n",
    "save_predictions(y_pred, './predictions/XGBoost_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be739867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
