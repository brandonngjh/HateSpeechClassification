{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8bf402",
   "metadata": {},
   "source": [
    "# Implementation of Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11d3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "train_df = pd.read_csv(\"./dataset/train_tfidf_features.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test_tfidf_features.csv\")\n",
    "\n",
    "X_train = train_df.drop([\"id\", \"label\"], axis=1)\n",
    "y_train = train_df[\"label\"]\n",
    "X_test = test_df.drop([\"id\"], axis=1)\n",
    "\n",
    "import os\n",
    "if not os.path.exists('./predictions'):\n",
    "    os.makedirs('./predictions')\n",
    "\n",
    "def cross_validation(model, X, y):\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring=make_scorer(f1_score, average='weighted'))\n",
    "    return scores.mean()\n",
    "\n",
    "def save_predictions(y_pred, filename):\n",
    "        output = pd.DataFrame({\"id\": test_df[\"id\"], \"label\": y_pred})\n",
    "        output.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cecc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for self-evaluation\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def f1_score(y_true, y_pred, class_label):\n",
    "    tp = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "    fp = np.sum((y_true != class_label) & (y_pred == class_label))\n",
    "    fn = np.sum((y_true == class_label) & (y_pred != class_label))\n",
    "    \n",
    "    if tp + 0.5 * (fp + fn) == 0:\n",
    "        return 0\n",
    "    \n",
    "    f1 = tp / (tp + 0.5 * (fp + fn))\n",
    "    return f1\n",
    "\n",
    "def macro_f1_score(y_true, y_pred):\n",
    "    f1_hateful = f1_score(y_true, y_pred, class_label=1)\n",
    "    f1_non_hateful = f1_score(y_true, y_pred, class_label=0)\n",
    "    return (f1_hateful + f1_non_hateful) / 2\n",
    "\n",
    "def self_evaluatation_training_set(y_train, y_train_pred):\n",
    "    print(\"Training set accuracy:\", accuracy(y_train, y_train_pred))\n",
    "    print(\"F1 Score for Hateful (class 1):\", f1_score(y_train, y_train_pred, 1))\n",
    "    print(\"F1 Score for Non-Hateful (class 0):\", f1_score(y_train, y_train_pred, 0))\n",
    "    print(\"Macro F1 Score:\", macro_f1_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c221b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: 2000\n",
      "Training set accuracy: 0.7832867783985102\n",
      "F1 Score for Hateful (class 1): 0.684085510688836\n",
      "F1 Score for Non-Hateful (class 0): 0.8350752878653676\n",
      "Macro F1 Score: 0.7595803992771017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: 1000\n",
      "Training set accuracy: 0.7543063314711359\n",
      "F1 Score for Hateful (class 1): 0.6357832988267771\n",
      "F1 Score for Non-Hateful (class 0): 0.8146294344924482\n",
      "Macro F1 Score: 0.7252063666596127\n",
      "Components: 500\n",
      "Training set accuracy: 0.7371973929236499\n",
      "F1 Score for Hateful (class 1): 0.60886887233674\n",
      "F1 Score for Non-Hateful (class 0): 0.8021207606695294\n",
      "Macro F1 Score: 0.7054948165031347\n",
      "Components: 100\n",
      "Training set accuracy: 0.6881401303538175\n",
      "F1 Score for Hateful (class 1): 0.4970436414828719\n",
      "F1 Score for Non-Hateful (class 0): 0.7740058195926285\n",
      "Macro F1 Score: 0.6355247305377502\n"
     ]
    }
   ],
   "source": [
    "# PCA dimensionality reduction and logistic regression\n",
    "# 2000 components seems to be too many features as indicated in task 2, reduce to test 1000, 500 and 100 for less time complexity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "components_list = [2000, 1000, 500, 100]\n",
    "log_reg_cv = LogisticRegressionCV(cv=5, random_state=0) # max_iter 1000 no dif\n",
    "\n",
    "for n_components in components_list:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    log_reg_cv.fit(X_train_pca, y_train)\n",
    "\n",
    "    y_pred = log_reg_cv.predict(X_test_pca)\n",
    "\n",
    "    # Self evaluation on training set\n",
    "    y_train_pred = log_reg_cv.predict(X_train_pca)\n",
    "    print(\"Components:\", n_components)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "    save_predictions(y_pred, f'./predictions/LogisticRegressionCV_PCA_{n_components}_components_Predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f663ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear\n",
      "Training set accuracy: 0.7314944134078212\n",
      "F1 Score for Hateful (class 1): 0.5715081723625557\n",
      "F1 Score for Non-Hateful (class 0): 0.8044915254237288\n",
      "Macro F1 Score: 0.6879998488931423\n",
      "Kernel: poly\n",
      "Training set accuracy: 0.8564944134078212\n",
      "F1 Score for Hateful (class 1): 0.7715397443023903\n",
      "F1 Score for Non-Hateful (class 0): 0.8953932298294731\n",
      "Macro F1 Score: 0.8334664870659316\n",
      "Kernel: rbf\n",
      "Training set accuracy: 0.8780842644320298\n",
      "F1 Score for Hateful (class 1): 0.8254893794252395\n",
      "F1 Score for Non-Hateful (class 0): 0.9063184724768591\n",
      "Macro F1 Score: 0.8659039259510493\n",
      "Kernel: sigmoid\n",
      "Training set accuracy: 0.6504888268156425\n",
      "F1 Score for Hateful (class 1): 0.4891988433407042\n",
      "F1 Score for Non-Hateful (class 0): 0.7343653250773994\n",
      "Macro F1 Score: 0.6117820842090518\n"
     ]
    }
   ],
   "source": [
    "# PCA dimensionality reduction and SVM, as SVMs typically perform better with high-dimensional and unstructured datasets, such as image and text data, compared to logistic regression.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pca = PCA(n_components=500)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel, random_state=0)\n",
    "    svm.fit(X_train_pca, y_train)\n",
    "    \n",
    "    y_pred = svm.predict(X_test_pca)\n",
    "\n",
    "    # Self evaluation on training set\n",
    "    y_train_pred = svm.predict(X_train_pca)\n",
    "    print(\"Kernel:\", kernel)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "    save_predictions(y_pred, f'./predictions/SVM_PCA_500_components_{kernel}_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier, no tuning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set. Shows a lot of overfitting\n",
    "y_train_pred = decision_tree.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "save_predictions(y_pred, f'./predictions/DecisionTree_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7867523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier with max_depth after considering overfitting. Performs worse\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth_values = [10, 100, 500]\n",
    "\n",
    "# Train and predict using Decision Tree for each max_depth value\n",
    "for max_depth in max_depth_values:\n",
    "    decision_tree = DecisionTreeClassifier(random_state=0, max_depth=max_depth)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    \n",
    "    save_predictions(y_pred, f'./predictions/DecisionTree_max_depth_{max_depth}_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7faf421a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Self evaluation on training set\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# y_train_pred = rf.predict(X_train)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# self_evaluatation_training_set(y_train, y_train_pred)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m f1_macro \u001b[38;5;241m=\u001b[39m make_scorer(f1_score, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf1_macro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault RF F1 Score (cross-validation):\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv_scores\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Random Forest Implementation, no tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "# y_train_pred = rf.predict(X_train)\n",
    "# self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "f1_macro = make_scorer(f1_score, average='macro')\n",
    "cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring=f1_macro)\n",
    "print(\"Default RF F1 Score (cross-validation):\", cv_scores.mean())\n",
    "\n",
    "# save_predictions(y_pred, './predictions/RandomForest_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f1adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954951515614263\n",
      "F1 Score for Non-Hateful (class 0): 0.9972262705091439\n",
      "Macro F1 Score: 0.9963607110352851\n",
      "100 Estimators - Training set accuracy: 0.996566573556797\n",
      "100 Estimators - F1 Score for Hateful (class 1): 0.9954951515614263\n",
      "100 Estimators - F1 Score for Non-Hateful (class 0): 0.9972262705091439\n",
      "100 Estimators - Macro F1 Score: 0.9963607110352851\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Implementation, no tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=0)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred = rf.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "save_predictions(y_pred, './predictions/RandomForest_500_Predictions.csv')\n",
    "\n",
    "# Random Forest with 100 estimators\n",
    "rf_1000 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf_1000.fit(X_train, y_train)\n",
    "y_pred_1000 = rf_1000.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred_100 = rf_1000.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "save_predictions(y_pred_1000, './predictions/RandomForest_1000_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e44d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Training set accuracy: 0.9917946927374302\n",
      "F1 Score for Hateful (class 1): 0.9892358195282083\n",
      "F1 Score for Non-Hateful (class 0): 0.9933706333160939\n",
      "Macro F1 Score: 0.9913032264221511\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, \n",
    "                                   n_iter=20, cv=3, scoring=scorer, verbose=2, random_state=0, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "save_predictions(y_pred, './predictions/RandomForest_Tuned_Predictions.csv')\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred = best_rf.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db4eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954903309638462\n",
      "F1 Score for Non-Hateful (class 0): 0.9972280949025135\n",
      "Macro F1 Score: 0.9963592129331799\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier, no tuning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set. Shows a lot of overfitting\n",
    "y_train_pred = decision_tree.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "save_predictions(y_pred, f'./predictions/DecisionTree_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272d0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest with 100 estimators, 100 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "\n",
      "Training Random Forest with 200 estimators, 100 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954937752997786\n",
      "F1 Score for Non-Hateful (class 0): 0.9972267920094007\n",
      "Macro F1 Score: 0.9963602836545896\n",
      "\n",
      "Training Random Forest with 300 estimators, 100 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954979015642884\n",
      "F1 Score for Non-Hateful (class 0): 0.9972252269200019\n",
      "Macro F1 Score: 0.9963615642421452\n",
      "\n",
      "Training Random Forest with 100 estimators, 500 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954972143783867\n",
      "F1 Score for Non-Hateful (class 0): 0.9972254878909005\n",
      "Macro F1 Score: 0.9963613511346436\n",
      "\n",
      "Training Random Forest with 200 estimators, 500 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "\n",
      "Training Random Forest with 300 estimators, 500 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "\n",
      "Training Random Forest with 100 estimators, 1000 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954958393770517\n",
      "F1 Score for Non-Hateful (class 0): 0.9972260096854577\n",
      "Macro F1 Score: 0.9963609245312547\n",
      "\n",
      "Training Random Forest with 200 estimators, 1000 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954958393770517\n",
      "F1 Score for Non-Hateful (class 0): 0.9972260096854577\n",
      "Macro F1 Score: 0.9963609245312547\n",
      "\n",
      "Training Random Forest with 300 estimators, 1000 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n"
     ]
    }
   ],
   "source": [
    "# PCA and random forest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_components_list = [100, 500, 1000]\n",
    "\n",
    "for n_components in n_components_list:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    n_estimators_list = [100, 200, 300]\n",
    "    \n",
    "    for n_estimators in n_estimators_list:\n",
    "        print(f\"\\nTraining Random Forest with {n_estimators} estimators, {n_components} components\")\n",
    "        \n",
    "        # Random Forest Implementation\n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
    "        rf.fit(X_train_pca, y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_test_pca)\n",
    "        \n",
    "        # Self evaluation on training set\n",
    "        y_train_pred = rf.predict(X_train_pca)\n",
    "        self_evaluatation_training_set(y_train, y_train_pred)\n",
    "        \n",
    "        # Save predictions\n",
    "        save_predictions(y_pred, f'./predictions/RandomForest_Predictions_{n_components}_components_{n_estimators}_estimators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28238fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\anaconda3\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:22:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7852071694599627\n",
      "F1 Score for Hateful (class 1): 0.660909508497933\n",
      "F1 Score for Non-Hateful (class 0): 0.8428224673167823\n",
      "Macro F1 Score: 0.7518659879073577\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting using XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "save_predictions(y_pred, './predictions/XGBoost_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dafa6f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying TSVD with 100 components\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mApplying TSVD with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m components\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m tsvd \u001b[38;5;241m=\u001b[39m TruncatedSVD(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m---> 11\u001b[0m X_train_tsvd \u001b[38;5;241m=\u001b[39m \u001b[43mtsvd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m X_test_tsvd \u001b[38;5;241m=\u001b[39m tsvd\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Logistic Regression with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m TSVD components\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:241\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m>\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be <=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m n_features(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m         )\n\u001b[1;32m--> 241\u001b[0m     U, Sigma, VT \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_svd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_oversamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_oversamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m VT\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# X @ V is not the same as U @ Sigma\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:446\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 446\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_range_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    455\u001b[0m B \u001b[38;5;241m=\u001b[39m safe_sparse_dot(Q\u001b[38;5;241m.\u001b[39mT, M)\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:282\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    278\u001b[0m         Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mqr(safe_sparse_dot(A\u001b[38;5;241m.\u001b[39mT, Q), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meconomic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mqr(\u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meconomic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TORUN: TVSD and Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components_list = [100, 500, 1000]\n",
    "\n",
    "for n_components in n_components_list:\n",
    "    print(f\"\\nApplying TSVD with {n_components} components\")\n",
    "    \n",
    "    tsvd = TruncatedSVD(n_components=n_components)\n",
    "    X_train_tsvd = tsvd.fit_transform(X_train)\n",
    "    X_test_tsvd = tsvd.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nTraining Logistic Regression with {n_components} TSVD components\")\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train_tsvd, y_train)\n",
    "    \n",
    "    y_pred = lr.predict(X_test_tsvd)\n",
    "    \n",
    "    # Self evaluation on training set\n",
    "    y_train_pred = lr.predict(X_train_tsvd)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_pred, f'./predictions/LogisticRegression_Predictions_{n_components}_components_TSVD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4cc19eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7540735567970205\n",
      "F1 Score for Hateful (class 1): 0.6373154823206316\n",
      "F1 Score for Non-Hateful (class 0): 0.8139637260081\n",
      "Macro F1 Score: 0.7256396041643658\n",
      "Training set accuracy: 0.8127327746741154\n",
      "F1 Score for Hateful (class 1): 0.7275651879444632\n",
      "F1 Score for Non-Hateful (class 0): 0.8573328604362476\n",
      "Macro F1 Score: 0.7924490241903555\n",
      "Training set accuracy: 0.9362779329608939\n",
      "F1 Score for Hateful (class 1): 0.9122947537044453\n",
      "F1 Score for Non-Hateful (class 0): 0.94996115706256\n",
      "Macro F1 Score: 0.9311279553835027\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = ['sigmoid', 'linear', 'rbf']\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = svm.predict(X_test)\n",
    "\n",
    "    #Self evaluation on training set\n",
    "    y_train_pred = svm.predict(X_train)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_test_pred, f\"./predictions/svm_{kernel}_predictions_default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e3cd4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning parameters for RBF kernel\n"
     ]
    }
   ],
   "source": [
    "# SVM Tuning. Took 8 hours and no result\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def self_evaluatation_training_set(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Self-evaluation accuracy on training set: {accuracy}\")\n",
    "\n",
    "# Define parameter grid for RBF kernel\n",
    "param_grid_rbf = {'C': [0.1, 1, 10, 100], 'gamma': [1e-3, 1e-2, 1e-1, 1]}\n",
    "\n",
    "# Perform Grid Search for RBF kernel\n",
    "print(\"Tuning parameters for RBF kernel\")\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "grid_search_rbf = GridSearchCV(svm_rbf, param_grid_rbf, cv=5, scoring='accuracy')\n",
    "grid_search_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model for RBF kernel\n",
    "best_params_rbf = grid_search_rbf.best_params_\n",
    "best_model_rbf = grid_search_rbf.best_estimator_\n",
    "print(f\"Best parameters for RBF kernel: {best_params_rbf}\")\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_test_pred_rbf = best_model_rbf.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred_rbf = best_model_rbf.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred_rbf)\n",
    "\n",
    "# Save predictions to a CSV file for RBF kernel\n",
    "filename_rbf = \"./predictions/svm_rbf_predictions_tuned.csv\"\n",
    "save_predictions(y_test_pred_rbf, filename_rbf)\n",
    "print(f\"Predictions saved to {filename_rbf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05942ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with n_estimators=180\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "Training Random Forest with n_estimators=220\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Implementation, varying estimators\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators_values = [180, 220]\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    print(f\"Training Random Forest with n_estimators={n_estimators}\")\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_pred, f\"./predictions/RandomForest_{n_estimators}_estimators_Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862774cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with max_depth=10\n",
      "Training set accuracy: 0.6213337988826816\n",
      "F1 Score for Hateful (class 1): 0.0133434420015163\n",
      "F1 Score for Non-Hateful (class 0): 0.7657077017246966\n",
      "Macro F1 Score: 0.38952557186310643\n",
      "Training Random Forest with max_depth=50\n",
      "Training set accuracy: 0.8119762569832403\n",
      "F1 Score for Hateful (class 1): 0.6755046700813498\n",
      "F1 Score for Non-Hateful (class 0): 0.8676416369669412\n",
      "Macro F1 Score: 0.7715731535241455\n",
      "Training Random Forest with max_depth=100\n",
      "Training set accuracy: 0.8948440409683427\n",
      "F1 Score for Hateful (class 1): 0.8403851249889586\n",
      "F1 Score for Non-Hateful (class 0): 0.9215950015186358\n",
      "Macro F1 Score: 0.8809900632537973\n",
      "Training Random Forest with max_depth=500\n",
      "Training set accuracy: 0.9962756052141527\n",
      "F1 Score for Hateful (class 1): 0.9951130116065975\n",
      "F1 Score for Non-Hateful (class 0): 0.9969913501316284\n",
      "Macro F1 Score: 0.996052180869113\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Implementation, varying max_depth. performs worse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_depth_values = [10, 50, 100, 500]\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    print(f\"Training Random Forest with max_depth={max_depth}\")\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=max_depth, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_pred, f\"./predictions/RandomForest_max_depth_{max_depth}_Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4b248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with criterion=entropy\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "Training Random Forest with criterion=log_loss\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Implementation, varying criterions. perform equal or worse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "criterions = [\"entropy\", \"log_loss\"]\n",
    "\n",
    "for criterion in criterions:\n",
    "    print(f\"Training Random Forest with criterion={criterion}\")\n",
    "    rf = RandomForestClassifier(n_estimators=200, criterion=criterion, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_pred, f\"./predictions/RandomForest_{criterion}_Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c41c53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default XGBoost F1 Score (cross-validation): 0.6667001387640876\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(eval_metric='error')\n",
    "\n",
    "f1_macro = make_scorer(f1_score, average='macro')\n",
    "cv_scores = cross_val_score(xgb, X_train, y_train, cv=5, scoring=f1_macro)\n",
    "print(\"Default XGBoost F1 Score (cross-validation):\", cv_scores.mean())\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "final_predictions = xgb.predict(X_test)\n",
    "save_predictions(final_predictions, './predictions/XGBoost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c51f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rfe = RFE(estimator=RandomForestClassifier(random_state=0), n_features_to_select=1000, step=10)\n",
    "X_train_selected = rfe.fit_transform(X_train, y_train)\n",
    "X_test_selected = rfe.transform(X_test)\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "\n",
    "print(f\"Number of selected features: {len(selected_features)}\")\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "rf_classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "test_predictions = rf_classifier.predict(X_test_selected)\n",
    "\n",
    "save_predictions(test_predictions, './predictions/rf_rfe_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e9e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
