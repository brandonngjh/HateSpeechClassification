{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb863c12",
   "metadata": {},
   "source": [
    "## Task 1: Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d295b2",
   "metadata": {},
   "source": [
    "### 1.1: Plain Logistic Regression. Score = 0.39226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c5ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9  ...  4990  4991  4992  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "\n",
      "   4993  4994  4995  4996  4997  4998  4999  \n",
      "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 5000 columns]\n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv files\n",
    "train_df = pd.read_csv('./dataset/train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('./dataset/test_tfidf_features.csv')\n",
    "\n",
    "X_train = train_df.drop(['id', 'label'], axis=1) # Features\n",
    "y_train = train_df['label'] # Labels\n",
    "X_test = test_df.drop(['id'], axis=1) # Test Features\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3befdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "def gradients(X, y, y_hat):\n",
    "    m = X.shape[0]\n",
    "    dw = (1/m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "def train(X, y, bs, epochs, lr):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "    y = y.values.reshape(m,1)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range((m - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # Calculate hypothesis\n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            # Getting gradients of loss\n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            \n",
    "            # Update parameters\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "            \n",
    "        # Calculating loss and appending to list\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(l)\n",
    "        \n",
    "    return w, b, losses\n",
    "\n",
    "def predict(X, w, b):\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d5f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "w, b, l = train(X_train, y_train, bs=64, epochs=100, lr=0.01)\n",
    "\n",
    "# Save predictions to CSV for the test set\n",
    "y_test_pred = predict(X_test, w, b)\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "predictions_df.to_csv('./predictions/LogRed_Prediction_Plain.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb3725",
   "metadata": {},
   "source": [
    "### 1.2: Modified Logistic Regression. Score = 0.62412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('./dataset/test_tfidf_features.csv')\n",
    "\n",
    "X_train = train_df.drop(['id', 'label'], axis=1) # Features\n",
    "y_train = train_df['label'] # Labels\n",
    "X_test = test_df.drop(['id'], axis=1) # Test Features\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Loss function with regularization\n",
    "def loss(y, y_hat, w, lambda_):\n",
    "    y_hat = np.clip(y_hat, 1e-10, 1-1e-10) # Avoid log(0)\n",
    "    log_loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    reg_loss = (lambda_ / 2) * np.sum(w**2) # L2 regularization\n",
    "    return log_loss + reg_loss\n",
    "\n",
    "# Gradients calculation with regularization\n",
    "def gradients(X, y, y_hat, w, lambda_):\n",
    "    m = X.shape[0]\n",
    "    dw = (1/m) * np.dot(X.T, (y_hat - y)) + (lambda_ / m) * w\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "def train(X, y, bs, epochs, lr, lambda_):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "    y = y.values.reshape(m, 1)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range((m - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            dw, db = gradients(xb, yb, y_hat, w, lambda_)\n",
    "            \n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "            \n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b), w, lambda_)\n",
    "        losses.append(l)\n",
    "        \n",
    "        # Convergence check\n",
    "        if epoch > 0 and np.abs(losses[-1] - losses[-2]) < 1e-6:\n",
    "            print(f'Converged at epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return w, b, losses\n",
    "\n",
    "def predict(X, w, b):\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    return np.array(pred_class)\n",
    "\n",
    "lambda_ = 0.01\n",
    "w, b, l = train(X_train, y_train, bs=64, epochs=1000, lr=0.01, lambda_=lambda_)\n",
    "\n",
    "y_test_pred = predict(X_test, w, b)\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "predictions_df.to_csv('./predictions/LogRed_Prediction.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470b659",
   "metadata": {},
   "source": [
    "### 1.3: SKLearn Logistic Regression for comparison. Score = 0.68446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ea8706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('./dataset/test_tfidf_features.csv')\n",
    "\n",
    "X_train = train_df.drop(['id', 'label'], axis=1)  # Features\n",
    "y_train = train_df['label']  # Labels\n",
    "X_test = test_df.drop(['id'], axis=1)  # Test Features\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({\"id\": test_df[\"id\"], \"label\": y_test_pred})\n",
    "output.to_csv(\"./predictions/SK_Learn_LogisticRegression_Predictions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
