{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f747f4e",
   "metadata": {},
   "source": [
    "# Hate Speech Classification Project Submission\n",
    "\n",
    "**50.007 Machine Learning - Summer 2024**\n",
    "\n",
    "**Group Name: Borky**\n",
    "\n",
    "**Task 1:**\n",
    "We implemented a plain logistic regression model and saved it as `LogRed_Prediction_Plain.csv`, which obtained a score of 0.39226. We also modified the implementation to better match and obtain similar performance to the scikit-learn logistic regression model by adding L2 regularization, convergence check, and increasing the number of epochs to allow the model to converge properly. This led to us obtaining a score of 0.62412 saved under `LogRed_Prediction`, comparable to the scikit-learn model which obtained 0.68446 under `SK_Learn_LogisticRegression_Predictions.csv`.\n",
    "\n",
    "**Task 2:**\n",
    "We experimented reducing dimensionality of the dataset using PCA and evaluating using KNN with the following number of principal components: 100, 500, 1000, and 2000. Our results showed that using 500 principal components yielded the highest F1 score of 0.57897, while the lowest performance was observed with 2000 components, scoring 0.50479. This indicates that a moderate reduction in dimensionality retains essential information, improving model accuracy.\n",
    "\n",
    "While higher numbers of components capture more variance, they also introduce computational inefficiency and potential overfitting. Optimal performance at 500 components highlights the balance between dimensionality reduction and retaining meaningful data which results in the KNN model being able to generalizes better.\n",
    "\n",
    "**Task 3:**\n",
    "We explored various machine learning models for classification from scikit-learn and XGB. Our best score of 0.71870 was achieved using a Voting Classifier combining Random Forest, Extra Trees, and Logistic Regression.\n",
    "\n",
    "**Results:**\n",
    "- **First Attempt Score:** 0.69386 (PCA + Logistic Regression)\n",
    "- **Random Forest Classifier Score:** 0.71254\n",
    "- **Extra Trees Classifier Score:** 0.71424\n",
    "- **Best Score (Voting Classifier):** 0.71870\n",
    "\n",
    "**Classifiers Experimented With and Tuned:**\n",
    "- Logistic Regression: Dimension reduction with PCA and TruncatedSVD\n",
    "- SVM: Dimension reduction with PCA\n",
    "- Decision Tree\n",
    "- Random Forest: Dimension reduction with PCA\n",
    "- AdaBoost\n",
    "- Extra Trees\n",
    "- GradientBoostingClassifier\n",
    "- XGBClassifier\n",
    "- Multinomial Naive Bayes\n",
    "- Complement Naive Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "- Voting Classifier of various other classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb863c12",
   "metadata": {},
   "source": [
    "## Task 1: Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d295b2",
   "metadata": {},
   "source": [
    "### 1.1: Plain Logistic Regression. Score = 0.39226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c5ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9  ...  4990  4991  4992  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "\n",
      "   4993  4994  4995  4996  4997  4998  4999  \n",
      "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 5000 columns]\n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv files\n",
    "train_df = pd.read_csv('./dataset/train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('./dataset/test_tfidf_features.csv')\n",
    "\n",
    "X_train = train_df.drop(['id', 'label'], axis=1) # Features\n",
    "y_train = train_df['label'] # Labels\n",
    "X_test = test_df.drop(['id'], axis=1) # Test Features\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3befdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "def gradients(X, y, y_hat):\n",
    "    m = X.shape[0]\n",
    "    dw = (1/m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "def train(X, y, bs, epochs, lr):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "    y = y.values.reshape(m,1)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range((m - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # Calculate hypothesis\n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            # Getting gradients of loss\n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            \n",
    "            # Update parameters\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "            \n",
    "        # Calculating loss and appending to list\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(l)\n",
    "        \n",
    "    return w, b, losses\n",
    "\n",
    "def predict(X, w, b):\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d5f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "w, b, l = train(X_train, y_train, bs=64, epochs=100, lr=0.01)\n",
    "\n",
    "# Save predictions to CSV for the test set\n",
    "y_test_pred = predict(X_test, w, b)\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "predictions_df.to_csv('./predictions/LogRed_Prediction_Plain.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb3725",
   "metadata": {},
   "source": [
    "### 1.2: Modified Logistic Regression. Score = 0.62412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('./dataset/test_tfidf_features.csv')\n",
    "\n",
    "X_train = train_df.drop(['id', 'label'], axis=1) # Features\n",
    "y_train = train_df['label'] # Labels\n",
    "X_test = test_df.drop(['id'], axis=1) # Test Features\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Loss function with regularization\n",
    "def loss(y, y_hat, w, lambda_):\n",
    "    y_hat = np.clip(y_hat, 1e-10, 1-1e-10) # Avoid log(0)\n",
    "    log_loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    reg_loss = (lambda_ / 2) * np.sum(w**2) # L2 regularization\n",
    "    return log_loss + reg_loss\n",
    "\n",
    "# Gradients calculation with regularization\n",
    "def gradients(X, y, y_hat, w, lambda_):\n",
    "    m = X.shape[0]\n",
    "    dw = (1/m) * np.dot(X.T, (y_hat - y)) + (lambda_ / m) * w\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "def train(X, y, bs, epochs, lr, lambda_):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "    y = y.values.reshape(m, 1)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range((m - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            dw, db = gradients(xb, yb, y_hat, w, lambda_)\n",
    "            \n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "            \n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b), w, lambda_)\n",
    "        losses.append(l)\n",
    "        \n",
    "        # Convergence check\n",
    "        if epoch > 0 and np.abs(losses[-1] - losses[-2]) < 1e-6:\n",
    "            print(f'Converged at epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return w, b, losses\n",
    "\n",
    "def predict(X, w, b):\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    return np.array(pred_class)\n",
    "\n",
    "lambda_ = 0.01\n",
    "w, b, l = train(X_train, y_train, bs=64, epochs=1000, lr=0.01, lambda_=lambda_)\n",
    "\n",
    "y_test_pred = predict(X_test, w, b)\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "predictions_df.to_csv('./predictions/LogRed_Prediction.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470b659",
   "metadata": {},
   "source": [
    "### 1.3: SKLearn Logistic Regression for comparison. Score = 0.68446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ea8706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('./dataset/test_tfidf_features.csv')\n",
    "\n",
    "X_train = train_df.drop(['id', 'label'], axis=1)  # Features\n",
    "y_train = train_df['label']  # Labels\n",
    "X_test = test_df.drop(['id'], axis=1)  # Test Features\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({\"id\": test_df[\"id\"], \"label\": y_test_pred})\n",
    "output.to_csv(\"./predictions/SK_Learn_LogisticRegression_Predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0510040",
   "metadata": {},
   "source": [
    "## Task 2: PCA Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c388b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = pd.read_csv(\"./dataset/train_tfidf_features.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test_tfidf_features.csv\")\n",
    "\n",
    "X_train = train_df.drop([\"id\", \"label\"], axis=1)\n",
    "y_train = train_df[\"label\"]\n",
    "X_test = test_df.drop([\"id\"], axis=1)\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b29ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: 2000, F1 Score: 0.4858\n",
      "Components: 1000, F1 Score: 0.5881\n",
      "Components: 500, F1 Score: 0.5954\n",
      "Components: 100, F1 Score: 0.6031\n",
      "Results Summary:\n",
      "Number of components: 2000, F1 Score: 0.48576472368733437\n",
      "Number of components: 1000, F1 Score: 0.588069875239095\n",
      "Number of components: 500, F1 Score: 0.5954067618257358\n",
      "Number of components: 100, F1 Score: 0.6031241190672758\n"
     ]
    }
   ],
   "source": [
    "components_list = [2000, 1000, 500, 100]\n",
    "results = []\n",
    "\n",
    "for n_components in components_list:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train_split)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "    neigh.fit(X_train_pca, y_train_split)\n",
    "    \n",
    "    y_val_pred = neigh.predict(X_val_pca)\n",
    "    \n",
    "    f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    results.append((n_components, f1))\n",
    "    print(f\"Components: {n_components}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    y_test_pred = neigh.predict(X_test_pca)\n",
    "\n",
    "    output = pd.DataFrame({\"id\": test_df[\"id\"], \"label\": y_test_pred})\n",
    "    output.to_csv(f\"./predictions/KNN_Predictions_{n_components}_components.csv\", index=False)\n",
    "\n",
    "print(\"Results Summary:\")\n",
    "for n_components, f1 in results:\n",
    "    print(f\"Number of components: {n_components}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8bf402",
   "metadata": {},
   "source": [
    "## Task 3: Implementation of Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325958e",
   "metadata": {},
   "source": [
    "### 3.0. Loading dataset and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "train_df = pd.read_csv(\"./dataset/train_tfidf_features.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test_tfidf_features.csv\")\n",
    "\n",
    "X_train = train_df.drop([\"id\", \"label\"], axis=1)\n",
    "y_train = train_df[\"label\"]\n",
    "X_test = test_df.drop([\"id\"], axis=1)\n",
    "\n",
    "import os\n",
    "if not os.path.exists('./predictions'):\n",
    "    os.makedirs('./predictions')\n",
    "\n",
    "def cross_validation(model, X, y):\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring=make_scorer(f1_score, average='weighted'))\n",
    "    return scores.mean()\n",
    "\n",
    "def save_predictions(y_pred, filename):\n",
    "        output = pd.DataFrame({\"id\": test_df[\"id\"], \"label\": y_pred})\n",
    "        output.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6cecc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for self-evaluation on training set\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def f1_score(y_true, y_pred, class_label):\n",
    "    tp = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "    fp = np.sum((y_true != class_label) & (y_pred == class_label))\n",
    "    fn = np.sum((y_true == class_label) & (y_pred != class_label))\n",
    "    \n",
    "    if tp + 0.5 * (fp + fn) == 0:\n",
    "        return 0\n",
    "    \n",
    "    f1 = tp / (tp + 0.5 * (fp + fn))\n",
    "    return f1\n",
    "\n",
    "def macro_f1_score(y_true, y_pred):\n",
    "    f1_hateful = f1_score(y_true, y_pred, class_label=1)\n",
    "    f1_non_hateful = f1_score(y_true, y_pred, class_label=0)\n",
    "    return (f1_hateful + f1_non_hateful) / 2\n",
    "\n",
    "def self_evaluatation_training_set(y_train, y_train_pred):\n",
    "    print(\"Training set accuracy:\", accuracy(y_train, y_train_pred))\n",
    "    print(\"F1 Score for Hateful (class 1):\", f1_score(y_train, y_train_pred, 1))\n",
    "    print(\"F1 Score for Non-Hateful (class 0):\", f1_score(y_train, y_train_pred, 0))\n",
    "    print(\"Macro F1 Score:\", macro_f1_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1cad2",
   "metadata": {},
   "source": [
    "### 3.1: Logistic Regression and SVM, with PCA and TSVD. Best score = 0.69597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c221b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA dimensionality reduction and logistic regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "components_list = [2000, 1000, 500, 100]\n",
    "log_reg_cv = LogisticRegressionCV(cv=5, random_state=0) # max_iter 1000 no dif\n",
    "\n",
    "for n_components in components_list:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    log_reg_cv.fit(X_train_pca, y_train)\n",
    "\n",
    "    y_pred = log_reg_cv.predict(X_test_pca)\n",
    "\n",
    "    # Self evaluation on training set\n",
    "    y_train_pred = log_reg_cv.predict(X_train_pca)\n",
    "    print(\"Components:\", n_components)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "    save_predictions(y_pred, f'./predictions/LogisticRegressionCV_PCA_{n_components}_components_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4cc19eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7540735567970205\n",
      "F1 Score for Hateful (class 1): 0.6373154823206316\n",
      "F1 Score for Non-Hateful (class 0): 0.8139637260081\n",
      "Macro F1 Score: 0.7256396041643658\n",
      "Training set accuracy: 0.8127327746741154\n",
      "F1 Score for Hateful (class 1): 0.7275651879444632\n",
      "F1 Score for Non-Hateful (class 0): 0.8573328604362476\n",
      "Macro F1 Score: 0.7924490241903555\n",
      "Training set accuracy: 0.9362779329608939\n",
      "F1 Score for Hateful (class 1): 0.9122947537044453\n",
      "F1 Score for Non-Hateful (class 0): 0.94996115706256\n",
      "Macro F1 Score: 0.9311279553835027\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = ['sigmoid', 'linear', 'rbf']\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = svm.predict(X_test)\n",
    "\n",
    "    #Self evaluation on training set\n",
    "    y_train_pred = svm.predict(X_train)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_test_pred, f\"./predictions/svm_{kernel}_predictions_default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f663ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear\n",
      "Training set accuracy: 0.7314944134078212\n",
      "F1 Score for Hateful (class 1): 0.5715081723625557\n",
      "F1 Score for Non-Hateful (class 0): 0.8044915254237288\n",
      "Macro F1 Score: 0.6879998488931423\n",
      "Kernel: poly\n",
      "Training set accuracy: 0.8564944134078212\n",
      "F1 Score for Hateful (class 1): 0.7715397443023903\n",
      "F1 Score for Non-Hateful (class 0): 0.8953932298294731\n",
      "Macro F1 Score: 0.8334664870659316\n",
      "Kernel: rbf\n",
      "Training set accuracy: 0.8780842644320298\n",
      "F1 Score for Hateful (class 1): 0.8254893794252395\n",
      "F1 Score for Non-Hateful (class 0): 0.9063184724768591\n",
      "Macro F1 Score: 0.8659039259510493\n",
      "Kernel: sigmoid\n",
      "Training set accuracy: 0.6504888268156425\n",
      "F1 Score for Hateful (class 1): 0.4891988433407042\n",
      "F1 Score for Non-Hateful (class 0): 0.7343653250773994\n",
      "Macro F1 Score: 0.6117820842090518\n"
     ]
    }
   ],
   "source": [
    "# PCA dimensionality reduction and SVM, as SVMs typically perform better with high-dimensional and unstructured datasets, such as image and text data, compared to logistic regression.\n",
    "# Tried multiple kernels, performed slightly better than Logistic Regression but took long to run\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pca = PCA(n_components=500)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel, random_state=0)\n",
    "    svm.fit(X_train_pca, y_train)\n",
    "    \n",
    "    y_pred = svm.predict(X_test_pca)\n",
    "\n",
    "    # Self evaluation on training set\n",
    "    y_train_pred = svm.predict(X_train_pca)\n",
    "    print(\"Kernel:\", kernel)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "    save_predictions(y_pred, f'./predictions/SVM_PCA_500_components_{kernel}_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TVSD and Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components_list = [100, 500, 1000]\n",
    "\n",
    "for n_components in n_components_list:\n",
    "    print(f\"\\nApplying TSVD with {n_components} components\")\n",
    "    \n",
    "    tsvd = TruncatedSVD(n_components=n_components)\n",
    "    X_train_tsvd = tsvd.fit_transform(X_train)\n",
    "    X_test_tsvd = tsvd.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nTraining Logistic Regression with {n_components} TSVD components\")\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train_tsvd, y_train)\n",
    "    \n",
    "    y_pred = lr.predict(X_test_tsvd)\n",
    "    \n",
    "    # Self evaluation on training set\n",
    "    y_train_pred = lr.predict(X_train_tsvd)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_pred, f'./predictions/LogisticRegression_Predictions_{n_components}_components_TSVD.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06738d41",
   "metadata": {},
   "source": [
    "### 3.2: Decision Tree. Best score = 0.67732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier, no tuning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set. Shows a lot of overfitting\n",
    "y_train_pred = decision_tree.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "save_predictions(y_pred, f'./predictions/DecisionTree_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7867523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier with max_depth after considering overfitting. Performs worse\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth_values = [10, 100, 500]\n",
    "\n",
    "# Train and predict using Decision Tree for each max_depth value\n",
    "for max_depth in max_depth_values:\n",
    "    decision_tree = DecisionTreeClassifier(random_state=0, max_depth=max_depth)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    \n",
    "    save_predictions(y_pred, f'./predictions/DecisionTree_max_depth_{max_depth}_Predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd88e97",
   "metadata": {},
   "source": [
    "### 3.3: Random Forest. Best score = 0.71254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Implementation, no tuning, first high score = 0.71254\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred = rf.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)\n",
    "\n",
    "save_predictions(y_pred, './predictions/RandomForest_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e44d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Training set accuracy: 0.9917946927374302\n",
      "F1 Score for Hateful (class 1): 0.9892358195282083\n",
      "F1 Score for Non-Hateful (class 0): 0.9933706333160939\n",
      "Macro F1 Score: 0.9913032264221511\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Hyperparameter tuning, performs worse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, \n",
    "                                   n_iter=20, cv=3, scoring=scorer, verbose=2, random_state=0, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "save_predictions(y_pred, './predictions/RandomForest_Tuned_Predictions.csv')\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred = best_rf.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest with 100 estimators, 100 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "\n",
      "Training Random Forest with 200 estimators, 100 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954937752997786\n",
      "F1 Score for Non-Hateful (class 0): 0.9972267920094007\n",
      "Macro F1 Score: 0.9963602836545896\n",
      "\n",
      "Training Random Forest with 300 estimators, 100 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954979015642884\n",
      "F1 Score for Non-Hateful (class 0): 0.9972252269200019\n",
      "Macro F1 Score: 0.9963615642421452\n",
      "\n",
      "Training Random Forest with 100 estimators, 500 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954972143783867\n",
      "F1 Score for Non-Hateful (class 0): 0.9972254878909005\n",
      "Macro F1 Score: 0.9963613511346436\n",
      "\n",
      "Training Random Forest with 200 estimators, 500 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "\n",
      "Training Random Forest with 300 estimators, 500 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "\n",
      "Training Random Forest with 100 estimators, 1000 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954958393770517\n",
      "F1 Score for Non-Hateful (class 0): 0.9972260096854577\n",
      "Macro F1 Score: 0.9963609245312547\n",
      "\n",
      "Training Random Forest with 200 estimators, 1000 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954958393770517\n",
      "F1 Score for Non-Hateful (class 0): 0.9972260096854577\n",
      "Macro F1 Score: 0.9963609245312547\n",
      "\n",
      "Training Random Forest with 300 estimators, 1000 components\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n"
     ]
    }
   ],
   "source": [
    "# PCA and random forest, performs worse\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_components_list = [100, 500, 1000]\n",
    "\n",
    "for n_components in n_components_list:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    n_estimators_list = [100, 200, 300]\n",
    "    \n",
    "    for n_estimators in n_estimators_list:\n",
    "        print(f\"\\nTraining Random Forest with {n_estimators} estimators, {n_components} components\")\n",
    "        \n",
    "        # Random Forest Implementation\n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
    "        rf.fit(X_train_pca, y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_test_pca)\n",
    "        \n",
    "        # Self evaluation on training set\n",
    "        y_train_pred = rf.predict(X_train_pca)\n",
    "        self_evaluatation_training_set(y_train, y_train_pred)\n",
    "        \n",
    "        # Save predictions\n",
    "        save_predictions(y_pred, f'./predictions/RandomForest_Predictions_{n_components}_components_{n_estimators}_estimators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05942ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with n_estimators=180\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n",
      "Training Random Forest with n_estimators=220\n",
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954965269826731\n",
      "F1 Score for Non-Hateful (class 0): 0.9972257488127145\n",
      "Macro F1 Score: 0.9963611378976938\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Implementation, varying estimators, performs worse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators_values = [180, 220]\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    print(f\"Training Random Forest with n_estimators={n_estimators}\")\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_pred, f\"./predictions/RandomForest_{n_estimators}_estimators_Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862774cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with max_depth=10\n",
      "Training set accuracy: 0.6213337988826816\n",
      "F1 Score for Hateful (class 1): 0.0133434420015163\n",
      "F1 Score for Non-Hateful (class 0): 0.7657077017246966\n",
      "Macro F1 Score: 0.38952557186310643\n",
      "Training Random Forest with max_depth=50\n",
      "Training set accuracy: 0.8119762569832403\n",
      "F1 Score for Hateful (class 1): 0.6755046700813498\n",
      "F1 Score for Non-Hateful (class 0): 0.8676416369669412\n",
      "Macro F1 Score: 0.7715731535241455\n",
      "Training Random Forest with max_depth=100\n",
      "Training set accuracy: 0.8948440409683427\n",
      "F1 Score for Hateful (class 1): 0.8403851249889586\n",
      "F1 Score for Non-Hateful (class 0): 0.9215950015186358\n",
      "Macro F1 Score: 0.8809900632537973\n",
      "Training Random Forest with max_depth=500\n",
      "Training set accuracy: 0.9962756052141527\n",
      "F1 Score for Hateful (class 1): 0.9951130116065975\n",
      "F1 Score for Non-Hateful (class 0): 0.9969913501316284\n",
      "Macro F1 Score: 0.996052180869113\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Implementation, varying max_depth. performs worse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_depth_values = [10, 50, 100, 500]\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    print(f\"Training Random Forest with max_depth={max_depth}\")\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=max_depth, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    self_evaluatation_training_set(y_train, y_train_pred)\n",
    "    \n",
    "    save_predictions(y_pred, f\"./predictions/RandomForest_max_depth_{max_depth}_Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with recursive feature elimination, performs worse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rfe = RFE(estimator=RandomForestClassifier(random_state=0), n_features_to_select=1000, step=10)\n",
    "X_train_selected = rfe.fit_transform(X_train, y_train)\n",
    "X_test_selected = rfe.transform(X_test)\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "\n",
    "print(f\"Number of selected features: {len(selected_features)}\")\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "rf_classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "test_predictions = rf_classifier.predict(X_test_selected)\n",
    "\n",
    "save_predictions(test_predictions, './predictions/rf_rfe_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51747407",
   "metadata": {},
   "source": [
    "### 3.4: Other Ensemble Methods. Best score = 0.71424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4e9e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7410381750465549\n",
      "F1 Score for Hateful (class 1): 0.6061946902654868\n",
      "F1 Score for Non-Hateful (class 0): 0.8070920756025664\n",
      "Macro F1 Score: 0.7066433829340266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# AdaBoost Implementation, no tuning\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred_ada = ada.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred_ada)\n",
    "\n",
    "save_predictions(y_pred_ada, './predictions/AdaBoost_Predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954903309638462\n",
      "F1 Score for Non-Hateful (class 0): 0.9972280949025135\n",
      "Macro F1 Score: 0.9963592129331799\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classifier. New Best Score = 0.71424\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_trees = ExtraTreesClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "extra_trees.fit(X_train, y_train)\n",
    "\n",
    "y_pred_extra_trees = extra_trees.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred_extra_trees = extra_trees.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred_extra_trees)\n",
    "\n",
    "save_predictions(y_pred_extra_trees, './predictions/ExtraTrees_Predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1319981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7321927374301676\n",
      "F1 Score for Hateful (class 1): 0.5319365337672904\n",
      "F1 Score for Non-Hateful (class 0): 0.8124388653407238\n",
      "Macro F1 Score: 0.6721876995540071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting Implementation, no tuning\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred_gb = gb.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred_gb)\n",
    "\n",
    "save_predictions(y_pred_gb, './predictions/GradientBoosting_Predictions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623d47e",
   "metadata": {},
   "source": [
    "### 3.5: Ensemble Methods using Voting Classifier. Best score = 0.71870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e60a765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954937752997786\n",
      "F1 Score for Non-Hateful (class 0): 0.9972267920094007\n",
      "Macro F1 Score: 0.9963602836545896\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier with RandomForest, ExtraTrees and LogisticRegression. Best score = 0.71870\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "et = ExtraTreesClassifier(n_estimators=200, random_state=0)\n",
    "lr = LogisticRegression(random_state=0)\n",
    "\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('et', et),\n",
    "    ('lr', lr)\n",
    "], voting='hard')\n",
    "\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "y_pred_voting = voting.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred_voting = voting.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred_voting)\n",
    "\n",
    "save_predictions(y_pred_voting, './predictions/Voting_Predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29b380af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954903309638462\n",
      "F1 Score for Non-Hateful (class 0): 0.9972280949025135\n",
      "Macro F1 Score: 0.9963592129331799\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier with RandomForesta and ExtraTrees.\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "et = ExtraTreesClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('et', et),\n",
    "], voting='hard')\n",
    "\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "y_pred_voting = voting.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred_voting = voting.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred_voting)\n",
    "\n",
    "save_predictions(y_pred_voting, './predictions/Voting_RF_ET_Predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4e52ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.996566573556797\n",
      "F1 Score for Hateful (class 1): 0.9954930868535635\n",
      "F1 Score for Non-Hateful (class 0): 0.997227052685999\n",
      "Macro F1 Score: 0.9963600697697812\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier with RandomForest, ExtraTrees and LogisticRegression. Best score = 0.71870\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "et = ExtraTreesClassifier(n_estimators=200, random_state=0)\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('et', et),\n",
    "    ('gb', gb)\n",
    "], voting='hard')\n",
    "\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "y_pred_voting = voting.predict(X_test)\n",
    "\n",
    "# Self evaluation on training set\n",
    "y_train_pred_voting = voting.predict(X_train)\n",
    "self_evaluatation_training_set(y_train, y_train_pred_voting)\n",
    "\n",
    "save_predictions(y_pred_voting, './predictions/Voting_RF_ET_GB_Predictions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030276c",
   "metadata": {},
   "source": [
    "### 3.6: Naive Bayes. Best score = 0.70821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3465976",
   "metadata": {},
   "source": [
    "** did not do Gaussian Naive Bayes because it is more suitable for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60d204",
   "metadata": {},
   "source": [
    "#1: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984c9173-e53a-440d-af09-7450c3967e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score for MultinomialNB: 0.6666332622584117\n",
      "Predictions saved for MultinomialNB.\n"
     ]
    }
   ],
   "source": [
    "# model: Multinomial Naive Bayes\n",
    "# key hyperparameter: alpha 1.0\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])\n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score for MultinomialNB: {macro_f1}\")\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('MultinomialNB_1.csv', index=False) #name is kaggle submission: task3_MultinomialNB.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99870893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 2.0}\n",
      "Macro F1 score on validation set: 0.6849255756869093\n"
     ]
    }
   ],
   "source": [
    "# Best Naive Bayes score = 0.70821\n",
    "# model: Multinomial Naive Bayes with GridSearchCV, SMOTE\n",
    "# key hyperparameter: \n",
    "# - alpha 0.1, 0.5, 1.0, 2.0 , best is 2.0\n",
    "# - cross validation fold 3\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])\n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data as labels are discrete\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model = MultinomialNB()\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0] \n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='f1_macro', cv=3, n_jobs=1)\n",
    "grid_search.fit(X_train_smote, y_train_smote) #hyperparameter tuning\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score on validation set: {macro_f1}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('MultinomialNB_2.csv', index=False) #name is kaggle submission: task3_MultinomialNB_simplified.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1913ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 4.0}\n",
      "Macro F1 score on validation set: 0.6865894030598448\n"
     ]
    }
   ],
   "source": [
    "# model: Multinomial Naive Bayes with GridSearchCV, SMOTE (improved)\n",
    "# key hyperparameter: \n",
    "# - alpha [0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 10.0] , best is 4.0\n",
    "# - cross validation fold 6 , best is 6 after testing  few values\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])\n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model = MultinomialNB()\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 10.0]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='f1_macro', cv=6, n_jobs=1)\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score on validation set: {macro_f1}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('MultinomialNB_best.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443b014",
   "metadata": {},
   "source": [
    "#2 Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc9d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score for ComplementNB: 0.687515848490134\n"
     ]
    }
   ],
   "source": [
    "# model: Complement Naive Bayes with SMOTE\n",
    "# key hyperparameter: \n",
    "# - alpha 1.0\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])\n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model = ComplementNB(alpha=1.0)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score for ComplementNB: {macro_f1}\")\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('ComplementNB_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e045ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 2.0}\n",
      "Macro F1 score on validation set: 0.6849255756869093\n"
     ]
    }
   ],
   "source": [
    "# model: Complement Naive Bayes with SMOTE\n",
    "# key hyperparameter: \n",
    "# - alpha 0.01, 0.1, 0.5, 1.0, 2.0 , best is 2.0\n",
    "# - cross validation fold 5\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])  \n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])  \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model = ComplementNB()\n",
    "param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 2.0]}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='f1_macro', cv=5, n_jobs=1)\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score on validation set: {macro_f1}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('ComplementNB_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e37e28da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 6.0}\n",
      "Macro F1 score on validation set: 0.6906421879781679\n"
     ]
    }
   ],
   "source": [
    "# model: Complement Naive Bayes with SMOTE (improved)\n",
    "# key hyperparameter: \n",
    "# - alpha [0.5, 1.0, 2.0, 4.0, 6.0] , best is 6.0\n",
    "# - cross validation fold 10 , best is 10 after testing few values\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])\n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model = ComplementNB()\n",
    "param_grid = {'alpha': [2.0, 4.0, 6.0, 8.0, 10.0]}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='f1_macro', cv=10, n_jobs=1)\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score on validation set: {macro_f1}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('ComplementNB_best.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73196f1e",
   "metadata": {},
   "source": [
    "#3 Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61c3d66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 4.0}\n",
      "Macro F1 score on validation set: 0.7043370429336944\n"
     ]
    }
   ],
   "source": [
    "# model: Bernoulli Naive Bayes with SMOTE\n",
    "# key hyperparameter: \n",
    "# - alpha [0.5, 1.0, 2.0, 4.0, 6.0] , best is 4.0\n",
    "# - cross validation fold 5\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])  \n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# Binarize the features\n",
    "binarizer = Binarizer()\n",
    "X_binarized = binarizer.fit_transform(X)\n",
    "X_test_binarized = binarizer.transform(X_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model = BernoulliNB()\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0, 4.0, 6.0]}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='f1_macro', cv=5, n_jobs=1)\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score on validation set: {macro_f1}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_binarized)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('BernoulliNB_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5c21bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 2.0}\n",
      "Macro F1 score on validation set: 0.6988315321643366\n"
     ]
    }
   ],
   "source": [
    "# model: Bernoulli Naive Bayes with SMOTE\n",
    "# key hyperparameter: \n",
    "# - alpha [0.5, 1.0, 2.0, 4.0, 6.0] , best is 2.0\n",
    "# - cross validation fold 10, best is 10 after testing few values\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])  \n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# Binarize the features\n",
    "binarizer = Binarizer()\n",
    "X_binarized = binarizer.fit_transform(X)\n",
    "X_test_binarized = binarizer.transform(X_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model = BernoulliNB()\n",
    "param_grid = {'alpha': [0.5, 1.0, 2.0, 4.0, 6.0]}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='f1_macro', cv=10, n_jobs=1)\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score on validation set: {macro_f1}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_binarized)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('BernoulliNB_best.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf83da",
   "metadata": {},
   "source": [
    "#4 All 3 Naive Bayes models: Multinomial, Complement and Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea875a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score for VotingClassifier with Naive Bayes models: 0.8428333829608325\n"
     ]
    }
   ],
   "source": [
    "# model: 3 Naive Bayes Models with SMOTE\n",
    "# key hyperparameter: \n",
    "# - alpha 1.0\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])\n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "binarizer = Binarizer()\n",
    "X_binarized = binarizer.fit_transform(X)\n",
    "X_test_binarized = binarizer.transform(X_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_bin, X_val_bin = train_test_split(X_binarized, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "X_train_bin_smote, y_train_bin_smote = smote.fit_resample(X_train_bin, y_train)\n",
    "\n",
    "model_multinomial = MultinomialNB()\n",
    "model_complement = ComplementNB()\n",
    "model_bernoulli = BernoulliNB()\n",
    "\n",
    "# Create an ensemble with VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('multinomial', model_multinomial),\n",
    "    ('complement', model_complement),\n",
    "    ('bernoulli', model_bernoulli)\n",
    "], voting='hard')  # 'hard' voting\n",
    "\n",
    "# ** fit models separately because BernoulliNB requires binarized features\n",
    "model_multinomial.fit(X_train_smote, y_train_smote)\n",
    "model_complement.fit(X_train_smote, y_train_smote)\n",
    "model_bernoulli.fit(X_train_bin_smote, y_train_bin_smote)\n",
    "\n",
    "\n",
    "y_val_pred = ensemble.fit(X_val, y_val).predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score for VotingClassifier with Naive Bayes models: {macro_f1}\")\n",
    "\n",
    "y_test_pred = ensemble.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('NaiveBayesEnsemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdb99df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MultinomialNB alpha: 2.0\n",
      "Best ComplementNB alpha: 2.0\n",
      "Best BernoulliNB alpha: 4.0\n",
      "Macro F1 score for VotingClassifier with Naive Bayes models: 0.6849255756869093\n"
     ]
    }
   ],
   "source": [
    "# model: 3 Naive Bayes Models with SMOTE\n",
    "# key hyperparameter: \n",
    "# - multinomial alpha [0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 10.0] , best is 2.0\n",
    "# - complement alpha [0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 10.0] , best is 2.0\n",
    "# - bernoulli alpha [0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 10.0] , best is 4.0\n",
    "# - cross validation fold 5\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "train_df = pd.read_csv('train_tfidf_features.csv')\n",
    "test_df = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "X = train_df.drop(columns=['id', 'label'])\n",
    "y = train_df['label']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "binarizer = Binarizer()\n",
    "X_binarized = binarizer.fit_transform(X)\n",
    "X_test_binarized = binarizer.transform(X_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_bin, X_val_bin = train_test_split(X_binarized, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "X_train_bin_smote, y_train_bin_smote = smote.fit_resample(X_train_bin, y_train)\n",
    "\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 8.0]}\n",
    "\n",
    "# Set up GridSearchCV for each Naive Bayes model\n",
    "grid_search_multinomial = GridSearchCV(MultinomialNB(), param_grid, scoring='f1_macro', cv=5, n_jobs=1)\n",
    "grid_search_complement = GridSearchCV(ComplementNB(), param_grid, scoring='f1_macro', cv=5, n_jobs=1)\n",
    "grid_search_bernoulli = GridSearchCV(BernoulliNB(), param_grid, scoring='f1_macro', cv=5, n_jobs=1)\n",
    "\n",
    "grid_search_multinomial.fit(X_train_smote, y_train_smote)\n",
    "grid_search_complement.fit(X_train_smote, y_train_smote)\n",
    "grid_search_bernoulli.fit(X_train_bin_smote, y_train_bin_smote)\n",
    "\n",
    "# Get the best models\n",
    "best_multinomial = grid_search_multinomial.best_estimator_\n",
    "best_complement = grid_search_complement.best_estimator_\n",
    "best_bernoulli = grid_search_bernoulli.best_estimator_\n",
    "\n",
    "print(f\"Best MultinomialNB alpha: {grid_search_multinomial.best_params_['alpha']}\")\n",
    "print(f\"Best ComplementNB alpha: {grid_search_complement.best_params_['alpha']}\")\n",
    "print(f\"Best BernoulliNB alpha: {grid_search_bernoulli.best_params_['alpha']}\")\n",
    "\n",
    "# Create an ensemble with VotingClassifier using the best models\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('multinomial', best_multinomial),\n",
    "    ('complement', best_complement),\n",
    "    ('bernoulli', best_bernoulli)\n",
    "], voting='hard')\n",
    "\n",
    "ensemble.fit(X_train_smote, y_train_smote)\n",
    "y_val_pred = ensemble.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Macro F1 score for VotingClassifier with Naive Bayes models: {macro_f1}\")\n",
    "\n",
    "y_test_pred = ensemble.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': y_test_pred})\n",
    "submission.to_csv('NaiveBayesEnsemble_best.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe7d004",
   "metadata": {},
   "source": [
    "### 3.7: Gradient Boosting. Best score = 0.68816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ac0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv('train_tfidf_features.csv')\n",
    "# df_text = pd.read_csv('train.csv')\n",
    "df_raw_comp = pd.read_csv('test_tfidf_features.csv')\n",
    "df_comp_text = pd.read_csv('test.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Get the features from the dataframe ignoring the first 2 columns (index, label). 1st row is auto ignored.\n",
    "df_features = df_raw.iloc[:, 2:]\n",
    "df_labels = df_raw.iloc[:, 1]\n",
    "\n",
    "# Get the features from the competition submittion dataset, ignoring the first column (index). 1st row is auto ignored.\n",
    "df_features_comp = df_raw_comp.iloc[:, 1:]\n",
    "\n",
    "# Convert the DataFrame to a numpy array\n",
    "features = df_features.to_numpy()\n",
    "labels = df_labels.to_numpy()[:,np.newaxis]\n",
    "\n",
    "features_comp = df_features_comp.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fded8",
   "metadata": {},
   "source": [
    "#### XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177575b",
   "metadata": {},
   "source": [
    "Used GrindSearch and RandomSearch to attempt to obtain the best hyperparameters.  \n",
    "\n",
    "The chosen hyperparameters for GridSearch were:  \n",
    "* learning rate\n",
    "* number of trees (estimators) created\n",
    "* max depth per tree\n",
    "* subsample (fraction of samples used to fit each tree)\n",
    "* colsample_bytree (fraction of features used to fit each tree)\n",
    "\n",
    "Due to RandomSearch running faster than GridSearch, the additional hyperparameters were added:\n",
    "* reg_alpha (l1 regularization term on weights)\n",
    "* reg_lambda (l2 regularization term on weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd41b9",
   "metadata": {},
   "source": [
    "#### Search for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(tree_method=\"hist\", objective='binary:logistic', n_jobs=4)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='f1', n_jobs=4)\n",
    "grid_search.fit(features, labels)\n",
    "print(\"Best parameters found by grid search:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee51945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'subsample': uniform(0.6, 1.0),\n",
    "    'colsample_bytree': uniform(0.6, 1.0),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(1, 2)\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(tree_method = \"hist\", device = \"cpu\", objective='binary:logistic', n_jobs=4)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=10, cv=3, scoring='f1', random_state=42, n_jobs=4)\n",
    "random_search.fit(features, labels)\n",
    "print(\"Best parameters found by random search:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3042233",
   "metadata": {},
   "source": [
    "Based on the results of GridSearch and RandomSearch, the following hyperparameters were obtained and used to train the XGB classifier:  \n",
    "* n_estimators=300, max_depth=7, learning_rate=0.1, reg_alpha=0, reg_lambda=1 = 0.680\n",
    "* n_estimators=500, max_depth=7, learning_rate=0.1, reg_alpha=0, reg_lambda=1 = 0.734\n",
    "* n_estimators=500, max_depth=10, learning_rate=0.1, reg_alpha=0, reg_lambda=1, subsample=0.8\n",
    "* n_estimators=781, max_depth=9, learning_rate=0.06247746602583892, reg_lambda=2.9475110376829186, subsample=0.8327713404303042, reg_alpha=0.04666566321361543, colsample_bytree=0.6230624250414157"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a551e60",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c99102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Explored PCA to reduce the number of features to 95% of the variance, though it did not seem improve the model's F1 score.\n",
    "pca = PCA(n_components=0.95)\n",
    "clf = XGBClassifier(n_estimators=500, max_depth=7, learning_rate=0.1, reg_alpha=0, reg_lambda=1, subsample=0.8, objective='binary:logistic')\n",
    "pipeline = Pipeline(steps=[('pca', pca), ('clf', clf)])\n",
    "pipeline.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "preds = pipeline.predict(features)\n",
    "f1 = f1_score(labels, preds)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0d5f0",
   "metadata": {},
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(features_comp)\n",
    "\n",
    "# copy df_test_text to df_submission\n",
    "df_submission = df_comp_text.copy()\n",
    "# Add the predictions to the df_submission DataFrame\n",
    "df_submission['label'] = predictions\n",
    "# remove the post column\n",
    "df_submission = df_submission.drop(columns=['post'])\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
